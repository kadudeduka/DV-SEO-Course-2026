# AI Coach - Intelligence Layer Integration Plan

**Version:** 1.0  
**Date:** 2025-01-29  
**Status:** Integration Plan

---

## Overview

This document outlines the integration of the new intelligence layer components into the existing AI Coach flow. The integration is designed to be **minimal, safe, and modular** while maintaining cost efficiency and service separation.

**New Components to Integrate**:
1. Enhanced Intent Classification (8 intents with priority rules)
2. Response Rules (per intent)
3. New System Prompt (senior trainer behavior)
4. Response Validation (per intent)
5. Enhanced Confidence & Escalation (low confidence handling)

**Constraints**:
- âœ… No database schema changes
- âœ… No RLS/security changes
- âœ… Cost-efficient (minimal additional LLM calls)
- âœ… Modular services (keep separation)

---

## Current Flow (Before Integration)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI COACH QUERY PROCESSING                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. [QueryProcessorService] Validate Query
   â””â”€> Check course allocation, length, format

2. [QueryProcessorService] Preprocess Query
   â””â”€> Normalize whitespace, punctuation

3. [LabStruggleDetectionService] Detect Lab Struggle (if lab-related)
   â””â”€> Check learner's lab performance

4. [QueryProcessorService] Classify Intent
   â””â”€> [LLMService] classifyIntent() â†’ 5 intents (simple)
       - course_content
       - navigation
       - lab_guidance
       - lab_struggle
       - out_of_scope

5. Check Out of Scope
   â””â”€> Return error if out_of_scope

6. [ContextBuilderService] Build Context
   â””â”€> Get learner progress, current chapter, etc.

7. [RetrievalService] Hybrid Search
   â””â”€> Semantic + keyword search for chunks

8. [ContextBuilderService] Filter Chunks by Access
   â””â”€> Filter based on progress/access

9. [ContextBuilderService] Prioritize Chunks
   â””â”€> Rank chunks by relevance

10. [ContextBuilderService] Select Chunks (token limit)
    â””â”€> Select top chunks within 2000 tokens

11. [AICoachService] Build System Prompt
    â””â”€> Simple prompt with trainer personalization

12. [LLMService] Generate Answer
    â””â”€> generateAnswer() â†’ GPT-4o-mini
        - Basic system prompt
        - Context chunks
        - Lab guidance rules (if needed)

13. [LLMService] Estimate Confidence
    â””â”€> Simple LLM-based confidence (0-1)

14. Extract References
    â””â”€> Map chunks to references

15. Store Query/Response
    â””â”€> Save to database

16. Check Escalation
    â””â”€> If confidence < 0.65 â†’ escalate

17. Return Response
    â””â”€> Return answer, references, confidence, etc.
```

---

## Enhanced Flow (After Integration)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI COACH QUERY PROCESSING (ENHANCED)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. [QueryProcessorService] Validate Query
   â””â”€> âœ… REUSED (no changes)

2. [QueryProcessorService] Preprocess Query
   â””â”€> âœ… REUSED (no changes)

3. [LabStruggleDetectionService] Detect Lab Struggle (if lab-related)
   â””â”€> âœ… REUSED (no changes)

4. [QueryProcessorService] Classify Intent âš¡ ENHANCED
   â””â”€> [LLMService] classifyIntent() â†’ 8 intents (enhanced)
       - factual / structural
       - conceptual / explanatory
       - example / practical
       - how_to / guided_learning
       - navigation / where_is
       - lab_guidance (no answers)
       - struggle / confusion
       - out_of_scope
       â””â”€> Uses priority rules and keyword signals
       â””â”€> Fallback strategy for edge cases

5. Check Out of Scope
   â””â”€> âœ… REUSED (no changes)

6. [ContextBuilderService] Build Context
   â””â”€> âœ… REUSED (no changes)

7. [RetrievalService] Hybrid Search
   â””â”€> âœ… REUSED (no changes)

8. [ContextBuilderService] Filter Chunks by Access
   â””â”€> âœ… REUSED (no changes)

9. [ContextBuilderService] Prioritize Chunks
   â””â”€> âœ… REUSED (no changes)

10. [ContextBuilderService] Select Chunks (token limit)
    â””â”€> âœ… REUSED (no changes)

11. [AICoachService] Build System Prompt âš¡ ENHANCED
    â””â”€> Load new system prompt template
        - Senior trainer persona
        - Intent-aware rules
        - Response structure templates
        - Zero hallucination rules
        - Confidence-aware language
        â””â”€> Inject: trainer personalization, course context, intent, chunks

12. [LLMService] Generate Answer âš¡ ENHANCED
    â””â”€> generateAnswer() â†’ GPT-4o-mini
        - Enhanced system prompt (from step 11)
        - Intent-specific instructions
        - Confidence-aware instructions (if low confidence expected)
        - Response structure templates

13. [LLMService] Estimate Confidence âš¡ ENHANCED
    â””â”€> Multi-factor confidence calculation
        - Semantic similarity (40%)
        - Completeness (30%)
        - Query clarity (20%)
        - Historical accuracy (10%)
        â””â”€> Returns 0-1 score

14. [AICoachService] Validate Response ðŸ†• NEW
    â””â”€> validateResponse(response, intent, question, context)
        - Intent-specific validation rules
        - Pattern detection (vague hedging, direct answers, etc.)
        - Word count validation
        - Reference validation
        â””â”€> Returns: { passed: boolean, failures: [] }

15. [AICoachService] Check Regeneration ðŸ†• NEW
    â””â”€> shouldRegenerate(validationResult, confidence)
        - Critical failures â†’ regenerate (max 3 attempts)
        - Multiple failures â†’ regenerate (max 2 attempts)
        - Low confidence + failure â†’ regenerate (max 2 attempts)
        â””â”€> If regenerate: loop back to step 12 with enhanced prompt

16. [AICoachService] Handle Low Confidence ðŸ†• NEW
    â””â”€> If confidence < 0.65:
        - Generate low-confidence response structure
        - Add partial context (if available)
        - Add uncertainty disclaimer
        - Add escalation notice
        â””â”€> Replace answer with structured low-confidence response

17. Extract References
    â””â”€> âœ… REUSED (no changes)

18. Store Query/Response
    â””â”€> âœ… REUSED (no changes)

19. Check Escalation âš¡ ENHANCED
    â””â”€> shouldEscalate(confidence, validationResult, context)
        - Low confidence (< 0.65) â†’ escalate
        - Critical validation failures â†’ escalate
        - Context insufficiency â†’ escalate
        â””â”€> Create escalation with full context

20. Return Response
    â””â”€> Return answer, references, confidence, validation, escalated, etc.
```

---

## Component Mapping

### 1. Intent Classification

**Current**: `llmService.classifyIntent()` â†’ 5 intents  
**Enhanced**: `llmService.classifyIntent()` â†’ 8 intents with priority rules

**Changes**:
- âœ… **Modify**: `lms/services/llm-service.js` â†’ `classifyIntent()` method
  - Update system prompt with 8 intent definitions
  - Add priority rules logic
  - Add keyword signal detection
  - Add fallback strategy
- âœ… **Reuse**: Same LLM call, same cost (GPT-3.5-turbo, ~10 tokens)

**Integration Point**: Step 4 in flow

---

### 2. System Prompt

**Current**: Simple prompt in `_buildSystemPrompt()`  
**Enhanced**: Full system prompt from `AI_COACH_SYSTEM_PROMPT_FINAL.txt`

**Changes**:
- âœ… **Modify**: `lms/services/ai-coach-service.js` â†’ `_buildSystemPrompt()` method
  - Load prompt template from file or constant
  - Inject placeholders: {TRAINER_PERSONALIZATION}, {COURSE_NAME}, {INTENT_TYPE}, etc.
  - Add intent-specific rules section
  - Add confidence-aware rules (if low confidence expected)
- âœ… **Reuse**: Same prompt building logic, just enhanced template
- âœ… **No cost impact**: Same tokens, better instructions

**Integration Point**: Step 11 in flow

---

### 3. Response Validation

**Current**: Basic check for direct lab answers  
**Enhanced**: Full intent-specific validation

**Changes**:
- âœ… **New File**: `lms/services/response-validation-service.js`
  - `validateResponse(response, intent, question, context)` â†’ validation result
  - Intent-specific validation functions
  - Pattern detection helpers
  - Word count validation
- âœ… **Modify**: `lms/services/ai-coach-service.js` â†’ `processQuery()` method
  - Add validation step after answer generation
  - Add regeneration logic if validation fails
- âœ… **Reuse**: No LLM calls, pure pattern matching (fast, free)

**Integration Point**: Step 14 in flow

---

### 4. Confidence & Escalation

**Current**: Simple threshold check  
**Enhanced**: Multi-factor confidence + low confidence handling

**Changes**:
- âœ… **Modify**: `lms/services/llm-service.js` â†’ `estimateConfidence()` method
  - Add multi-factor calculation
  - Add semantic similarity scoring
  - Add completeness estimation
  - Add query clarity estimation
- âœ… **Modify**: `lms/services/ai-coach-service.js` â†’ `processQuery()` method
  - Add low confidence response generation
  - Add confidence-aware response structure
  - Enhance escalation logic
- âœ… **Reuse**: Same escalation service, enhanced decision logic
- âœ… **Cost impact**: Minimal (confidence calculation already exists, just enhanced)

**Integration Point**: Steps 13, 16, 19 in flow

---

### 5. Response Rules

**Current**: Basic lab guidance rules in system prompt  
**Enhanced**: Full intent-specific response rules

**Changes**:
- âœ… **Integrated into**: System prompt (step 11)
  - Response rules are part of system prompt template
  - Intent-specific rules injected based on classified intent
- âœ… **No separate service needed**: Rules are prompt-level
- âœ… **Reuse**: Same prompt injection mechanism

**Integration Point**: Step 11 in flow (via system prompt)

---

## Detailed Integration Points

### Integration Point 1: Enhanced Intent Classification

**File**: `lms/services/llm-service.js`  
**Method**: `classifyIntent(question, context)`

**Current Implementation**:
```javascript
// Returns: 'course_content' | 'navigation' | 'lab_guidance' | 'lab_struggle' | 'out_of_scope'
```

**Enhanced Implementation**:
```javascript
// Returns: 'factual' | 'conceptual' | 'example' | 'how_to' | 'navigation' | 
//          'lab_guidance' | 'struggle' | 'out_of_scope'

// Changes:
1. Update system prompt with 8 intent definitions
2. Add priority rules (check struggle before lab_guidance, etc.)
3. Add keyword signal detection
4. Add fallback strategy
```

**Cost Impact**: âœ… None (same LLM call, same model, same tokens)

---

### Integration Point 2: Enhanced System Prompt

**File**: `lms/services/ai-coach-service.js`  
**Method**: `_buildSystemPrompt(courseId, learnerId, intent, labStruggle)`

**Current Implementation**:
```javascript
// Simple prompt with basic rules
let basePrompt = `You are an AI Coach...`;
// Add trainer personalization
// Add basic rules
```

**Enhanced Implementation**:
```javascript
// Load full system prompt template
const systemPromptTemplate = loadSystemPromptTemplate(); // From AI_COACH_SYSTEM_PROMPT_FINAL.txt

// Replace placeholders:
- {TRAINER_PERSONALIZATION} â†’ trainerInfo
- {COURSE_NAME} â†’ courseName
- {COURSE_ID} â†’ courseId
- {INTENT_TYPE} â†’ intent (factual, conceptual, etc.)
- {INTENT_RULES} â†’ intent-specific rules section
- {CONTEXT_CHUNKS} â†’ selected chunks (injected in LLM call)
- {LEARNER_CONTEXT} â†’ progress context
- {LAB_STRUGGLE_CONTEXT} â†’ lab struggle (if applicable)
- {CONFIDENCE_AWARE_RULES} â†’ low confidence rules (if confidence expected to be low)
```

**Cost Impact**: âœ… Minimal (slightly longer prompt, but same tokens per response)

---

### Integration Point 3: Response Validation

**File**: `lms/services/response-validation-service.js` (NEW)  
**Method**: `validateResponse(response, intent, question, context)`

**Implementation**:
```javascript
// Pure JavaScript validation (no LLM calls)
// Pattern matching, word count, reference checks
// Returns: { passed: boolean, failures: [], wordCount: number }
```

**Integration in `ai-coach-service.js`**:
```javascript
// After answer generation:
const validation = await responseValidationService.validateResponse(
    answerResult.answer,
    intent,
    processedQuestion,
    { chunks: selectedChunks, context: fullContext }
);

// Check if regeneration needed:
const regeneration = shouldRegenerate(validation, answerResult.confidence);

if (regeneration.regenerate && attempts < regeneration.maxAttempts) {
    // Regenerate with enhanced prompt + validation feedback
    const enhancedPrompt = buildRegenerationPrompt(systemPrompt, validation, answerResult.answer);
    // Loop back to answer generation
}
```

**Cost Impact**: âœ… None (pure JavaScript, no LLM calls)

---

### Integration Point 4: Enhanced Confidence Calculation

**File**: `lms/services/llm-service.js`  
**Method**: `estimateConfidence(question, contextChunks, answer)`

**Current Implementation**:
```javascript
// Simple LLM-based confidence estimation
// Returns: 0-1 score
```

**Enhanced Implementation**:
```javascript
// Multi-factor confidence:
1. Semantic Similarity (40%): avgSimilarity from chunks
2. Completeness (30%): estimateCompleteness(question, answer, chunks)
3. Query Clarity (20%): estimateQueryClarity(question)
4. Historical Accuracy (10%): getHistoricalAccuracy(question) [optional, can default to 0.5]

// Weighted average
const confidence = (
    similarityScore * 0.4 +
    completenessScore * 0.3 +
    clarityScore * 0.2 +
    historicalScore * 0.1
);
```

**Cost Impact**: âœ… Minimal (can keep existing LLM-based estimation or use pure JS)

---

### Integration Point 5: Low Confidence Response Handling

**File**: `lms/services/ai-coach-service.js`  
**Method**: `processQuery()` â†’ Add new method `_generateLowConfidenceResponse()`

**Implementation**:
```javascript
// After confidence calculation:
if (answerResult.confidence < 0.65) {
    // Generate low confidence response structure
    const lowConfidenceResponse = this._generateLowConfidenceResponse(
        answerResult.answer,  // Partial context (if any)
        processedQuestion,
        answerResult.confidence,
        selectedChunks
    );
    
    // Replace answer
    answerResult.answer = lowConfidenceResponse;
    answerResult.isLowConfidence = true;
}
```

**Cost Impact**: âœ… None (pure string manipulation, no LLM calls)

---

### Integration Point 6: Enhanced Escalation Logic

**File**: `lms/services/ai-coach-service.js`  
**Method**: `processQuery()` â†’ Enhance escalation check

**Current Implementation**:
```javascript
const shouldEscalate = answerResult.confidence < this.confidenceThreshold;
```

**Enhanced Implementation**:
```javascript
const shouldEscalate = this._shouldEscalate(
    answerResult.confidence,
    validation,  // From response validation
    { chunks: selectedChunks, context: fullContext }
);

// _shouldEscalate() checks:
// 1. Low confidence (< 0.65) â†’ escalate
// 2. Critical validation failures â†’ escalate
// 3. Context insufficiency â†’ escalate
```

**Cost Impact**: âœ… None (pure logic, no LLM calls)

---

## Service Structure (After Integration)

```
lms/services/
â”œâ”€â”€ ai-coach-service.js âš¡ ENHANCED
â”‚   â”œâ”€â”€ processQuery() â†’ Enhanced with validation, regeneration, low confidence handling
â”‚   â”œâ”€â”€ _buildSystemPrompt() â†’ Enhanced with new template
â”‚   â”œâ”€â”€ _generateLowConfidenceResponse() â†’ NEW
â”‚   â”œâ”€â”€ _shouldEscalate() â†’ Enhanced
â”‚   â””â”€â”€ _buildRegenerationPrompt() â†’ NEW
â”‚
â”œâ”€â”€ llm-service.js âš¡ ENHANCED
â”‚   â”œâ”€â”€ classifyIntent() â†’ Enhanced with 8 intents, priority rules
â”‚   â”œâ”€â”€ generateAnswer() â†’ Enhanced with new system prompt
â”‚   â””â”€â”€ estimateConfidence() â†’ Enhanced with multi-factor calculation
â”‚
â”œâ”€â”€ response-validation-service.js ðŸ†• NEW
â”‚   â”œâ”€â”€ validateResponse() â†’ Main validation function
â”‚   â”œâ”€â”€ validateFactualResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateConceptualResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateExampleResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateHowToResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateNavigationResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateLabGuidanceResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateStruggleResponse() â†’ Intent-specific
â”‚   â”œâ”€â”€ validateOutOfScopeResponse() â†’ Intent-specific
â”‚   â””â”€â”€ [Helper functions] â†’ Pattern detection, word count, etc.
â”‚
â”œâ”€â”€ query-processor-service.js âœ… REUSED (no changes)
â”œâ”€â”€ context-builder-service.js âœ… REUSED (no changes)
â”œâ”€â”€ retrieval-service.js âœ… REUSED (no changes)
â”œâ”€â”€ escalation-service.js âœ… REUSED (no changes)
â”œâ”€â”€ lab-struggle-detection-service.js âœ… REUSED (no changes)
â””â”€â”€ trainer-personalization-service.js âœ… REUSED (no changes)
```

---

## Cost Analysis

### Current Cost Per Query

1. **Intent Classification**: 1 call to GPT-3.5-turbo (~10 tokens) = ~$0.00001
2. **Answer Generation**: 1 call to GPT-4o-mini (~500 tokens) = ~$0.00015
3. **Confidence Estimation**: 1 call to GPT-3.5-turbo (~10 tokens) = ~$0.00001
4. **Total**: ~$0.00017 per query

### Enhanced Cost Per Query

1. **Intent Classification**: 1 call to GPT-3.5-turbo (~15 tokens) = ~$0.00001 âš¡ Same
2. **Answer Generation**: 1 call to GPT-4o-mini (~600 tokens) = ~$0.00018 âš¡ +20% (longer prompt)
3. **Confidence Estimation**: 1 call to GPT-3.5-turbo (~10 tokens) = ~$0.00001 âš¡ Same (or pure JS)
4. **Response Validation**: 0 LLM calls = $0 âœ… Free
5. **Regeneration** (if needed): +1 call to GPT-4o-mini (~600 tokens) = ~$0.00018 âš¡ Only if validation fails
6. **Total**: ~$0.00020 per query (base) + $0.00018 if regeneration needed

**Cost Impact**: âœ… **Minimal** (~18% increase base, +100% only if regeneration needed)

**Regeneration Rate**: Expected < 10% of queries â†’ Average cost increase ~25%

---

## Safety & Risk Mitigation

### 1. Backward Compatibility

âœ… **Intent Classification**: Map old intents to new intents for compatibility
```javascript
const intentMap = {
    'course_content': 'factual',  // Default mapping
    'navigation': 'navigation',
    'lab_guidance': 'lab_guidance',
    'lab_struggle': 'struggle',
    'out_of_scope': 'out_of_scope'
};
```

âœ… **System Prompt**: Fallback to old prompt if new template fails to load

âœ… **Validation**: Validation failures don't block responses (log and continue)

### 2. Error Handling

âœ… **Validation Errors**: Log and continue (don't block user)
âœ… **Regeneration Failures**: Max attempts (3) then return best response
âœ… **Low Confidence Handling**: Always provide partial context if available

### 3. Gradual Rollout

âœ… **Feature Flags**: Enable/disable new features per environment
```javascript
const FEATURES = {
    ENHANCED_INTENT_CLASSIFICATION: true,
    RESPONSE_VALIDATION: true,
    LOW_CONFIDENCE_HANDLING: true,
    ENHANCED_ESCALATION: true
};
```

---

## Implementation Order

### Phase 1: Core Enhancements (Low Risk)
1. âœ… Enhanced Intent Classification (8 intents)
2. âœ… Enhanced System Prompt (new template)
3. âœ… Enhanced Confidence Calculation

**Risk**: Low  
**Impact**: High  
**Cost**: Minimal

### Phase 2: Validation Layer (Medium Risk)
4. âœ… Response Validation Service
5. âœ… Regeneration Logic

**Risk**: Medium (new logic)  
**Impact**: High (quality improvement)  
**Cost**: Minimal (only if validation fails)

### Phase 3: Low Confidence Handling (Low Risk)
6. âœ… Low Confidence Response Generation
7. âœ… Enhanced Escalation Logic

**Risk**: Low  
**Impact**: High (better user experience)  
**Cost**: None

---

## Testing Strategy

### Unit Tests
- âœ… Intent classification (8 intents, priority rules)
- âœ… Response validation (each intent type)
- âœ… Confidence calculation (multi-factor)
- âœ… Low confidence response generation
- âœ… Escalation logic

### Integration Tests
- âœ… End-to-end query processing with new flow
- âœ… Regeneration loop (max attempts)
- âœ… Low confidence handling
- âœ… Escalation creation

### Regression Tests
- âœ… Existing queries still work
- âœ… Backward compatibility maintained
- âœ… No performance degradation

---

## Summary

### What's Reused (No Changes)
- âœ… Query validation
- âœ… Query preprocessing
- âœ… Lab struggle detection
- âœ… Context building
- âœ… Chunk retrieval
- âœ… Chunk filtering/prioritization
- âœ… Database storage
- âœ… Escalation service (interface)

### What's Enhanced (Minimal Changes)
- âš¡ Intent classification (8 intents, priority rules)
- âš¡ System prompt (new template, same injection)
- âš¡ Confidence calculation (multi-factor, same interface)
- âš¡ Escalation logic (enhanced decision, same service)

### What's New (Modular Services)
- ðŸ†• Response validation service (pure JS, no LLM)
- ðŸ†• Regeneration logic (in ai-coach-service)
- ðŸ†• Low confidence response generation (in ai-coach-service)

### Cost Impact
- âœ… Base: ~18% increase (longer prompt)
- âœ… With regeneration: ~25% average increase
- âœ… Validation: Free (no LLM calls)

### Risk Level
- âœ… Low (modular, backward compatible, feature flags)

---

**Document Status**: âœ… Ready for Implementation

